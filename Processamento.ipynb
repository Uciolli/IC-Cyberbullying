{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c1fea08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to C:\\Users\\Uciolli\n",
      "[nltk_data]     Betelli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Uciolli\n",
      "[nltk_data]     Betelli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('rslp')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import codecs\n",
    "import unicodedata\n",
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29669be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP_Kit(object):\n",
    "\t#construtor da classe\n",
    "\tdef __init__(self, stopwordsFile = None):\n",
    "\t\t'''Caso voce queira passar um arquivo de stopwords proprio,\n",
    "\t\tutilize-o durante a construcao do objeto NLP_kit com seu caminho\n",
    "\t\tabsoluto'''\n",
    "\t\tself.stemmer = nltk.stem.RSLPStemmer()\n",
    "\t\tself.sent_tokenizer = None\n",
    "\t\tself.stopwords = None\n",
    "\t\tself.stopwordsFile = stopwordsFile\n",
    "\n",
    "\tdef normaliza(self, texto):\n",
    "\t\t'''Retira acentos do texto e o retorna\n",
    "\t\t'''\n",
    "\t\ttry:\n",
    "\t\t\treturn unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore')\n",
    "\t\texcept:\n",
    "\t\t\treturn unicodedata.normalize('NFKD', texto.decode('UTF-8')).encode('ASCII', 'ignore')\n",
    "\t\traise Exception('Erro no formato da string. Utilize unicode utf-8')\n",
    "\n",
    "\tdef separa_frases(texto):\n",
    "\t\t'''\n",
    "\t\tRecebe um texto. Retorna lista com as frases contidas no texto que possuem mais\n",
    "\t\tde 1 caracter\n",
    "\t\t'''\n",
    "\t\t#adiciona espacos entre pontuacoes, para facilitar o trabalho do sent_tokenizer.\n",
    "\t\ttexto = re.sub('([.,!?()])', r' \\1 ', texto)\n",
    "\t\ttexto = re.sub('\\s{2,}', ' ', texto)\n",
    "\n",
    "\t\tif self.sent_tokenizer is None:\n",
    "\t\t\tself.sent_tokenizer = nltk.data.load('tokenizers/punkt/portuguese.pickle')\n",
    "\t\tsentencas =  self.sent_tokenizer.tokenize(texto)\n",
    "\t\tsentencas = [sentenca for sentenca in sentencas if len(sentenca) > 1]\n",
    "\t\treturn sentencas\n",
    "\n",
    "\tdef tokenize(self, frase):\n",
    "\t\t'''Recebe uma frase. Retorna lista de tokens (palavras) da frase\n",
    "\t\tque sao separadas por espaco ou virgula, travessao etc\n",
    "\t\tRetorna em Unicode e minusculo\n",
    "\t\t'''\n",
    "\t\tfrase = frase.lower()\n",
    "\t\tif not isinstance(frase, str):\n",
    "\t\t\tfrase = frase.decode('utf-8')\n",
    "\t\treturn re.findall(r'\\w+', frase,flags = re.UNICODE)\n",
    "\n",
    "\tdef stemiza(palavra):\n",
    "\t\t'''Recebe uma palavra. Retorna seu stem\n",
    "\t\t'''\n",
    "\t\treturn nltk.stemmer.stem(palavra)\n",
    "\n",
    "\tdef remove_stop_words(self, texto):\n",
    "\t\t'''Retorna lista de palavras sem stopwords \n",
    "\t\t'''\n",
    "\t\tif self.stopwords is None:\n",
    "\t\t\tif self.stopwordsFile is not None:\n",
    "\t\t\t\tstopwords = [word for word in codecs.open(stopwordsFile, 'r', 'utf-8')]\n",
    "\t\t\telse:\n",
    "\t\t\t\tstopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "\t\t\tstopwords = [str( self.normaliza(item), \"utf-8\" ) for item in stopwords]\n",
    "\n",
    "\t\t\tpalavras = self.tokenize(texto)\n",
    "\t\t\t\n",
    "\t\t\t#print type(stopwords[1])\n",
    "\t\t\tpalavras_importantes = [palavra for palavra in palavras if palavra.lower() not in stopwords]\n",
    "\t\treturn palavras_importantes\n",
    "\n",
    "\tdef palavras_mais_frequentes(self, texto):\n",
    "\t\t'''retorna lista de tuplas <palavra, frequencia>\n",
    "\t\t'''\n",
    "\t\tpalavras = self.remove_stop_words(texto)\n",
    "\n",
    "\t\tfd = nltk.FreqDist(palavras)\n",
    "\t\tfrequencias = []\n",
    "\t\tfor palavra in fd:\n",
    "\t\t\tfrequencias.append( (palavra, fd[palavra]) )\n",
    "\t\treturn frequencias\n",
    "\n",
    "\tdef palavras_mais_frequentes2(self, texto): #com stopwords\n",
    "\t\t'''retorna lista de tuplas <palavra, frequencia>\n",
    "\t\t'''\n",
    "\t\tpalavras = texto.split()\n",
    "\n",
    "\t\tfd = nltk.FreqDist(palavras)\n",
    "\t\tfrequencias = []\n",
    "\t\tfor palavra in fd:\n",
    "\t\t\tfrequencias.append( (palavra, fd[palavra]) )\n",
    "\t\treturn frequencias\n",
    "\n",
    "\tdef remove_palavra_stop_words(self, texto):#retorna frase\n",
    "\t\t'''Retorna palavras sem stopwords \n",
    "\t\t'''\n",
    "\t\tif self.stopwords is None:\n",
    "\t\t\tif self.stopwordsFile is not None:\n",
    "\t\t\t\tstopwords = [word for word in codecs.open(stopwordsFile, 'r', 'utf-8')]\n",
    "\t\t\telse:\n",
    "\t\t\t\tstopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "\t\t\tstopwords = [str( self.normaliza(item), \"utf-8\" ) for item in stopwords]\n",
    "\t\t\t#print(stopwords)    #mostra quais sao os stopwords retirados\n",
    "\t\t\tpalavras = texto.split(' ')\n",
    "\n",
    "\t\t\tfrase_sem_sw= ''\n",
    "\t\t\t\n",
    "\t\t\tfor palavra in palavras:\n",
    "\t\t\t\tif palavra not in stopwords:\n",
    "\t\t\t\t\tfrase_sem_sw += palavra+' '\n",
    "\t\treturn frase_sem_sw\n",
    "\n",
    "\tdef remove_pontuacao(self, texto):#retorna frase\n",
    "\t\tstring_normalizada = re.sub(':|;|\\[|\\]|\\(|\\)|,|\\.|!|\\?|\\{|\\}|\\*|\\\"|-', ' ', texto.replace('\\n',' '))\t#retira pontuacao\n",
    "\t\tstring_normalizada = string_normalizada.replace('//',' ')\n",
    "\t\tstring_normalizada = string_normalizada.replace('\\'','')\n",
    "\t\treturn string_normalizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0985714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#texto = \"Teste De verificação :) de um Tweet teste!\"\n",
    "#frases = NLP_Kit()\n",
    "#string_normalizada = frases.remove_pontuacao(texto)\n",
    "#print(string_normalizada)\n",
    "#string_normalizada = frases.normaliza(string_normalizada)   #retira acentos\n",
    "#string_normalizada = str( string_normalizada, \"utf-8\" )\n",
    "#print(string_normalizada)\n",
    "#string_normalizada = frases.remove_palavra_stop_words(string_normalizada.lower())\n",
    "#print(string_normalizada)\n",
    "#string_normalizada = re.sub(' +', ' ', string_normalizada) #remove espacos em branco multiplos\n",
    "#print(string_normalizada)\n",
    "#string_normalizada = ((unicodedata.normalize('NFKD', string_normalizada)).encode('ASCII', 'ignore').lower())\n",
    "#print(string_normalizada)\n",
    "#string_normalizada = frases.palavras_mais_frequentes(string_normalizada)\n",
    "#print(string_normalizada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abec9de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv\n",
    "#with open('tweets_ofensivos_sem_repeticao.csv', encoding=\"utf8\") as f:\n",
    "#    reader = csv.reader(f)\n",
    "#    lista = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2a58eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def processa_texto_base(caminho_in, caminho_out):\n",
    "    with open(caminho_in, encoding=\"utf8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        lista = list(reader)\n",
    "        \n",
    "    lista_processada = []\n",
    "    frases = NLP_Kit()\n",
    "    for row in lista:\n",
    "        texto_processando = frases.remove_pontuacao(row[0])\n",
    "        texto_processando = frases.normaliza(texto_processando)\n",
    "        texto_processando = str( texto_processando, \"utf-8\" )\n",
    "        texto_processando = frases.remove_palavra_stop_words(texto_processando.lower())\n",
    "        texto_processando = re.sub(' +', ' ', texto_processando)\n",
    "        lista_processada.append(texto_processando)\n",
    "\n",
    "    dict_lista = {'text': lista_processada}\n",
    "    df = pd.DataFrame(dict_lista)\n",
    "    df.to_csv(caminho_out, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36969d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "processa_texto_base('csvs/pre-processado/tweets_odio_bolsonaro_texto.csv', 'csvs/processados/tweets_odio_bolsonaro_processado.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6468b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processa_texto_base('csvs/pre-processado/tweets_odio_lula_texto.csv', 'csvs/processados/tweets_odio_lula_processado.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "40c80f1f4ea213d5622cb53e73a85b2cd22909bbd70666763ad1f6c0ff157a19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
